# 如果没有link，需要留空
# Technique can be selected from: Algorithm-level, System-level, Model-level (Quantization), Model-level (Sparsification), Model-level (Structure Optimization)
# Target can be selected from: Efficient Inference, Efficient Training, Efficient Optimization Process
# Domain can be selected from: Language, Vision Generation, Vision Recognition, 3D Modeling, Other
# special_label can be anything. Currently, there are three types of special labels: Survey, Evaluation, Benchmark
# support links: paper_link, code_link, slide_link, website_link, video_link


- title: '(Chinese Book) Efficient Deep Learning: Model Compression and Design. 《高效深度学习：模型压缩与设计》 (京东有售)'
  authors: Yu Wang, Xuefei Ning
  conference: Publishing House of Electronics Industry 2024
  technique: Model-level
  target: Efficient Inference
  domain: Vision Recognition, Vision Generation, Language

- title: 'Distilling Auto-regressive Models into Few Steps 1: Image Generation'
  authors: Enshu Liu, Xuefei Ning+, Yu Wang, Zinan Lin+
  conference: ArXiv 2024
  technique: Algorithm-level
  target: Efficient Inference
  domain: Vision Generation

- title: 'GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration'
  authors: Kaiyi Huang, Yukun Huang, Xuefei Ning, Zinan Lin, Yu Wang, Xihui Liu+
  conference: ArXiv 2024
  paper_link: "https://arxiv.org/abs/2412.04440"
  website_link: "https://karine-h.github.io/GenMAC/"
  code_link: "https://github.com/Karine-Huang/GenMAC"
  video_link: "https://www.youtube.com/watch?v=gni-nzJQ9TI"
  technique: 
  target: Better Application
  domain: Vision Generation

- title: 'Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding'
  authors: Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, Xihui Liu+
  conference: ArXiv 2024
  paper_link: "https://arxiv.org/abs/2410.01699"
  technique: Algorithm-level
  target: Efficient Inference
  domain: Vision Generation

- title: 'Training-Free and Hardware-Friendly Acceleration for Diffusion Models via Similarity-based Token Pruning'
  authors: Evelyn Zhang, Jiayi Tang, Xuefei Ning, Linfeng Zhang
  conference: AAAI 2025
  paper_link: "https://openreview.net/forum?id=q7q1o78MRA"
  technique: Model-level
  target: Efficient Inference
  domain: Vision Generation

- title: 'Rad-NeRF: Ray-decoupled Training of Neural Radiance Field'
  authors: Lidong Guo*, Xuefei Ning*+, Yonggan Fu, Tianchen Zhao, Zhuoliang Kang, Jincheng Yu, Yingyan Celine Lin, Yu Wang+
  conference: NeurIPS 2024
  paper_link: "https://openreview.net/forum?id=nBrnfYeKf9"
  code_link: "https://github.com/thu-nics/Rad-NeRF"
  video_link: "https://www.bilibili.com/video/BV124SMYcEVk/"
  technique: Algorithm-level
  target: Better Application
  domain: 3D Modeling

- title: 'Can LLMs Learn by Teaching for Better Reasoning? A Preliminary Study'
  authors: Xuefei Ning*+, Zifu Wang*, Shiyao Li*, Zinan Lin*+, Peiran Yao*, Tianyu Fu, Matthew B. Blaschko, Guohao Dai, Huazhong Yang, Yu Wang+
  conference: NeurIPS 2024
  paper_link: "https://arxiv.org/abs/2406.14629"
  code_link: "https://github.com/imagination-research/lbt"
  website_link: "https://sites.google.com/view/llm-learning-by-teaching"
  video_link: "https://www.bilibili.com/video/BV1f6UoYQEqw/"
  technique: Algorithm-level
  target: Better Reasoning
  domain: Language
  descrip: 'This study explores whether or not the current LLMs can learn by teach (LbT), which is a well-recognized paradigm in human learning. As one can imagine, the ability of LbT could offer exciting opportunities for the models to continuously evolve by teaching other (potentially weaker) models. We implement the LbT idea into well-established pipelines to see if it can improve the reasoning outcomes and ability on complex tasks (e.g., mathematical reasoning, competition-level code synthesis). The results show some promise, and importantly, we share our thoughts on the research rationale and roadmap in detail.'

- title: 'Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models: Enhancing Performance and Reducing Inference Costs'
  authors: Enshu Liu*, Junyi Zhu*, Zinan Lin+, Xuefei Ning+, Matthew B. Blaschko, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang+
  conference: ArXiv 2024
  paper_link: "https://arxiv.org/abs/2407.00945"
  code_link: "https://github.com/imagination-research/EEP"
  technique: Model-level (Pruning)
  target: Efficient Inference
  domain: Language

- title: "MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression"
  authors: Tianyu Fu*, Haofeng Huang*, Xuefei Ning*+, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang
  conference: ArXiv 2024
  paper_link: "https://arxiv.org/abs/2406.14909"
  code_link: "https://github.com/thu-nics/MoA"
  website_link: "https://thu-nics.github.io/MoA_project_page/"
  technique: Model-level (Sparsification)
  target: Efficient Inference
  domain: Language
  descrip: 'Mixture of Sparse Attention (MoA) addresses the computational and memory challenges of long-context LLM inference. It proposes an automatic compression pipeline, assigning the optimal heterogeneous elastic sparse pattern for each attention head. MoA achieves a 1.2-1.4x GPU memory reduction, boosting decode throughput by 6.6−8.2x and 1.7−1.9x compared to FlashAttention2 and vLLM, with minimal impact on performance.'

- title: "DiTFastAttn: Attention Compression for Diffusion Transformer Models"
  authors: Zhihang Yuan*, Pu Lu*, Hanling Zhang*, Xuefei Ning+, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, Yu Wang
  conference: NeurIPS 2024
  paper_link: "https://arxiv.org/pdf/2406.08552"
  code_link: "https://github.com/thu-nics/DiTFastAttn"
  website_link: "https://nics-effalg.com/DiTFastAttn"
  video_link: "https://www.bilibili.com/video/BV1VQUVYxEfx/"
  technique: Model-level (Sparsification), Model-level (Structure Optimization)
  target: Efficient Inference
  domain: Vision Generation
  descrip: 'Diffusion Transformers (DiT) excel at image and video generation but face computational challenges due to the quadratic complexity of self-attention operators. We identify three types of redundancies in DiT and propose DiTFastAttn, a post-training compression method, to reduce them. Our method reduces up to 76% of the attention FLOPs and achieves up to 1.8x end-to-end speedup at high-resolution (2k x 2k) generation.'


- title: "ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation"
  authors: Tianchen Zhao, Tongcheng Fang, Enshu Liu, Wan Rui, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning+, Yu Wang+
  conference: ArXiv 2024
  paper_link: "https://arxiv.org/pdf/2406.02540"
  code_link: "https://github.com/thu-nics/ViDiT-Q"
  website_link: https://nics-effalg.com/ViDiT_Q
  technique: Model-level (Quantization)
  target: Efficient Inference
  domain: Vision Generation
  descrip: This paper investigates the quantization of diffusion transformers, conducting a systematic analysis of the sources of quantization error. It then designs a unique static-dynamic channel balancing technique to address the time-varying channel imbalance problem. Additionally, a metric-decoupled mixed precision approach is adopted to handle failures under lower bitwidth settings (W4A8, W4A4). This method achieves lossless W4A8 quantization for a variety of popular text-to-image and text-to-video models.

- title: "DiM: Diffusion Mamba for Efficient High-Resolution Image Synthesis"
  authors: Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, Xihui Liu+
  conference: ArXiv 2024
  paper_link: "https://arxiv.org/abs/2405.14224"
  code_link: "https://github.com/tyshiwo1/DiM-DiffusionMamba/"
  technique: Model-level (Structure Optimization)
  target: Efficient Inference
  domain: Vision Generation

- title: "Linear Combination of Saved Checkpoints Makes Consistency and Diffusion Models Better"
  authors: Enshu Liu*, Junyi Zhu*, Zinan Lin+, Xuefei Ning+, Matthew B. Blaschko, Sergey Yekhanin, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang+
  conference: ArXiv 2024
  paper_link: "https://arxiv.org/pdf/2404.02241"
  code_link: "https://github.com/imagination-research/LCSC"
  technique: Algorithm-level
  target: Efficient Training, Efficient Inference
  domain: Vision Generation

- title: "MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization"
  authors: Tianchen Zhao*, Xuefei Ning*+, Tongcheng Fang*, Enshu Liu, Guyue Huang, Zinan Lin, Shengen Yan, Guohao Dai, and Yu Wang+
  conference: ECCV 2024
  paper_link: "https://arxiv.org/pdf/2405.17873"
  code_link: "https://github.com/thu-nics/MixDQ"
  website_link: "https://a-suozhang.xyz/mixdq.github.io/"
  video_link: "https://www.bilibili.com/video/BV1cozuYPEaE/"
  technique: Model-level (Quantization)
  target: Efficient Inference
  domain: Vision Generation
  descrip: This paper addresses the issue of existing quantization techniques faces challenge when quantizing the "few-step" diffusion models. It introduces the BOS-aware quantization technique to handle the highly sensitive text embedding related layers, and proposes a novel metric decoupled sensitivity analysis method to decouple the effect of quantization on image quality and content. MixDQ achieves lossless W4A8 quantization for challenging one-step SDXL-turbo model, while existing method fall short at W8A8.

- title: "A Survey on Efficient Inference for Large Language Models"
  authors: Zixuan Zhou*, Xuefei Ning*+, Ke Hong*, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai+, Xiao-Ping Zhang, Yuhan Dong, Yu Wang+
  conference: ArXiv 2024
  paper_link: "https://arxiv.org/pdf/2404.14294"
  target: Efficient Inference
  domain: Language
  special_label: Survey

- title: "LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K"
  authors: Tao Yuan, Xuefei Ning+, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai+, Shengen Yan, Yu Wang+
  conference: ArXiv 2024
  paper_link: "https://arxiv.org/abs/2402.05136"
  code_link: "https://github.com/infinigence/LVEval"
  website_link: "https://huggingface.co/datasets/Infinigence/LVEval"
  special_label: Benchmark, Evaluation

- title: "Towards Floating Point-Based Attention-Free LLM: Hybrid PIM with Non-Uniform Data Format and Reduced Multiplications"
  authors: Lidong Guo*, Zhenhua Zhu*+, Tengxuan Liu, Xuefei Ning, Shiyao Li, Guohao Dai, Huazhong Yang, Wangyang Fu and Yu Wang+
  conference: ICCAD 2024
  paper_link: https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/3b18c2b6-a7c4-439d-9fa8-d6276749f085.pdf
  technique: System-level
  target: Efficient Inference
  domain: Language

- title: "FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs"
  authors: Shulin Zeng*, Jun Liu*, Guohao Dai+, Xinhao Yang, Tianyu Fu, Hongyi Wang, Wenheng Ma, Hanbo Sun, Shiyao Li, Zixiao Huang, Yadong Dai, Jintao Li, Zehao Wang, Ruoyu Zhang, Kairui Wen, Xuefei Ning, Yu Wang+
  conference: "FPGA 2024"
  paper_link: https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/3fa036e2-0d62-45c4-81e2-20d9b7189504.pdf
  technique: System-level, Model-level (Quantization), Model-level (Sparsification)
  target: "Efficient Inference"
  domain: "Language" 

- title: "DyPIM: Dynamic-inference-enabled Processing-In-Memory Accelerator"
  authors: Tongxin Xie, Tianchen Zhao, Zhenhua Zhu, Xuefei Ning, Bing Li, Guohao Dai, Huazhong Yang, Yu Wang
  conference: "DATE 2024"
  paper_link: "https://ieeexplore.ieee.org/abstract/document/10546612"
  technique: System-level
  target: "Efficient Inference"
  domain: "Vision Recognition"

- title: "Evaluating Quantized Large Language Models"
  authors: Shiyao Li, Xuefei Ning+, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang+
  conference: ICML 2024
  paper_link: "https://arxiv.org/pdf/2402.18158"
  code_link: "https://github.com/thu-nics/qllm-eval"
  video_link: "https://www.bilibili.com/video/BV1Kb421H7uH/"
  technique: Model-level (Quantization)
  target: Efficient Inference
  domain: Language
  special_label: Evaluation
  descrip: We evaluate the performance of 11 LLM families under W, WA, KV quantization on various tasks. Based on the evaluation results, we summarize tensor-level, model-level, and task-level knowledge for quantized LLMs.

- title: "FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models"
  authors: Lin Zhao*, Tianchen Zhao*, Zinan Lin+, Xuefei Ning+, Guohao Dai, Huazhong Yang, Yu Wang+
  conference: CVPR 2024
  paper_link: "https://arxiv.org/pdf/2403.16379"
  code_link: "https://github.com/thu-nics/FlashEval"
  video_link: "https://www.bilibili.com/video/BV1Cz421b7Ft/"
  technique: Algorithm-level
  target: Efficient Optimization Process
  domain: Vision Generation
  descrip: This paper addresses the issue of the evaluation process for text-to-image generation models, which often involves generating images on 1K-4K prompts, making it cumbersome for application scenarios requiring iterative evaluation. It constructs an extensive model zoo and introduces an evolutionary-inspired searching method to identify a "representative subset" of textual datasets. With equivalent evaluation quality (measured using Kendall's tau), FlashEval's 50-item prompt set can replace a 500-item randomly sampled subset, achieving a 10x speedup in evaluation efficiency.

- title: "A Unified Sampling Framework for Solver Searching of Diffusion Probabilistic Models"
  authors: Enshu Liu, Xuefei Ning+, Huazhong Yang, Yu Wang+
  conference: ICLR 2024
  paper_link: "https://arxiv.org/pdf/2312.07243"
  code_link: "https://github.com/thu-nics/USF"
  video_link: "https://www.bilibili.com/video/BV17z421b79T/"
  technique: Algorithm-level
  target: Efficient Inference
  domain: Vision Generation

- title: "Skeleton-of-Thought: Prompting Large Language Models for Efficient Parallel Generation"
  authors: Xuefei Ning*+, Zinan Lin*, Zixuan Zhou*, Zifu Wang, Huazhong Yang, Yu Wang+
  conference: ICLR 2024
  paper_link: https://arxiv.org/abs/2307.15337
  code_link: "https://github.com/imagination-research/sot"
  video_link: "https://www.bilibili.com/video/BV1EM4m1S7ZH/"
  technique: Algorithm-level
  target: Efficient Inference
  domain: Language

- title: "TCP: Triplet Contrastive-relationship Preserving for Class-Incremental Learning"
  authors: Shiyao Li, Xuefei Ning+, Shanghang Zhang, Lidong Guo, Tianchen Zhao, Huazhong Yang, Yu Wang+
  conference: WACV 2024
  paper_link: "https://openaccess.thecvf.com/content/WACV2024/papers/Li_TCP_Triplet_Contrastive-Relationship_Preserving_for_Class-Incremental_Learning_WACV_2024_paper.pdf"

- title: "LLM-MQ: Mixed-precision Quantization for Efficient LLM Deployment"
  authors: Shiyao Li, Xuefei Ning+, Ke Hong, Tengxuan Liu, Luning Wang, Xiuhong Li, Kai Zhong, Guohao Dai, Huazhong Yang, Yu Wang+
  conference: NeurIPS Workshop 2023
  paper_link: "https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/5c805adc-b555-499f-9882-5ca35ce674b5.pdf"
  code_link: 
  technique: Model-level (Quantization)
  target: Efficient Inference
  domain: Language

- title: "Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels"
  authors: Zifu Wang, Xuefei Ning, Matthew B. Blaschko
  conference: NeurIPS 2023
  paper_link: "https://proceedings.neurips.cc/paper_files/paper/2023/file/ee208bfc04b1bf6125a6a34baa1c28d3-Paper-Conference.pdf"
  code_link: "https://github.com/zifuwanggg/JDTLosses"
  domain: Vision Recognition

- title: "Ada3D: Exploiting the Spatial Redundancy with Adaptive Inference for Efficient 3D Object Detection"
  authors: Tianchen Zhao, Xuefei Ning+, Ke Hong, Zhongyuan Qiu, Pu Lu, Linfeng Zhang, Yali Zhao, Lipu Zhou, Guohao Dai, Huazhong Yang, Yu Wang+
  conference: ICCV 2023
  paper_link: "https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Ada3D__Exploiting_the_Spatial_Redundancy_with_Adaptive_Inference_for_ICCV_2023_paper.pdf"
  video_link: "https://www.bilibili.com/video/BV1vC4y1S7FW/"
  technique: Model-level (Sparsification)
  target: Efficient Inference
  domain: Vision Recognition

- title: "OMS-DPM: Deciding The Optimal Model Schedule for Diffusion Probabilistic Model"
  authors: Enshu Liu*, Xuefei Ning*+, Zinan Lin*, Huazhong Yang, Yu Wang+
  conference: ICML 2023
  paper_link: "https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2Ff34b14d9-6e99-4fc6-ae98-3e4c7c966ef9.pdf"
  code_link: "https://github.com/jsttlgdkycy/OMS-DPM"
  website_link: "https://sites.google.com/view/oms-dpm/"
  video_link: "https://www.bilibili.com/video/BV1hC4y1D7oa/"
  technique: Algorithm-level
  target: Efficient Inference
  domain: Vision Generation

- title: 'Dynamic Ensemble of Low-fidelity Experts: Mitigating NAS "Cold-Start"'
  authors: Junbo Zhao*, Xuefei Ning*+, Enshu Liu, Binxin Ru, Zixuan Zhou, Tianchen Zhao, Chen Chen, Jiajin Zhang, Qingmin Liao, Yu Wang+
  conference: AAAI 2023 (Oral)
  paper_link: "https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/4208e529-772e-4977-be31-0b7cc4c7a9fc.pdf"
  code_link: "https://github.com/A-LinCui/DELE"
  technique: Model-level (Structure Optimization)
  target: Efficient Optimization Process
  domain: Vision Recognition

- title: "Memory-Oriented Structural Pruning for Efficient Image Restoration"
  authors: Xiangsheng Shi*, Xuefei Ning*+, Lidong Guo*, Tianchen Zhao, Enshu Liu, Yi Cai, Yuhan Dong, Huazhong Yang, Yu Wang+
  conference: AAAI 2023
  paper_link: "https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2F8ed3ecfa-b1ea-4230-88a3-14eca04c87ef.pdf"
  code_link: 
  technique: Model-level (Structure Optimization)
  target: Efficient Inference
  domain: Vision Generation

- title: "Ensemble-in-One: Ensemble Learning within Random Gated Networks for Enhanced Adversarial Robustness"
  authors: Yi Cai, Xuefei Ning, Huazhong Yang, Yu Wang
  conference: AAAI 2023
  paper_link: "https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2F8ed3ecfa-b1ea-4230-88a3-14eca04c87ef.pdf"
  technique: 
  target: 
  domain: Vision Recognition

- title: "A Generic Graph-based Neural Architecture Encoding Scheme with Multifaceted Information"
  authors: Xuefei Ning, Yin Zheng, Zixuan Zhou, Tianchen Zhao, Huazhong Yang, Yu Wang
  conference: TPAMI 2023
  paper_link: "https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/a10f869e-e43c-43bb-9be5-6c574a344bfc.pdf"
  code_link: "https://github.com/walkerning/aw_nas"
  technique: Model-level (Structure Optimization)
  target: Efficient Optimization Process
  domain: Vision Recognition

- title: "Gibbon: Efficient Co-Exploration of NN Model and Processing-In-Memory Architecture"
  authors: Hanbo Sun*, Chenyu Wang*, Zhenhua Zhu, Xuefei Ning+, Guohao Dai, Huazhong Yang, Yu Wang+
  conference: DATE 2022 & TCAD 2023
  paper_link: "https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/b6a3130a-52cc-4896-9cb5-d34b56adc968.pdf"
  code_link: 
  technique: Model-level (Structure Optimization), System-level
  target: Efficient Optimization Process, Efficient Inference
  domain: Vision Recognition

- title: "Exploring the Potential of Low-bit Training of Convolutional Neural Networks"
  authors: Kai Zhong, Xuefei Ning, Guohao Dai, Zhenhua Zhu, Tianchen Zhao, Shulin Zeng, Yu Wang+, Huazhong Yang
  conference: TCAD 2022
  paper_link: "https://arxiv.org/pdf/2006.02804"
  code_link: 
  technique: Model-level (Quantization)
  target: Efficient Training
  domain: Vision Recognition

- title: "CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance"
  authors: Tianchen Zhao, Niansong Zhang, Xuefei Ning, He Wang, Li Yi, Yu Wang
  conference: CVPR 2022
  paper_link: "https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2F4e19ce30-1cb0-4447-91b2-ed2da43238e9.pdf"
  code_link: 
  technique: Model-level (Structure Optimization)
  target: Efficient Inference
  domain: Vision Recognition

- title: "FedCor: Correlation-Based Active Client Selection Strategy for Heterogeneous Federated Learning"
  authors: Minxue Tang, Xuefei Ning, Yitu Wang, Jingwei Sun, Yu Wang, Hai Li, Yiran Chen+
  conference: CVPR 2022
  paper_link: "https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/2a74b4a9-8673-47b2-8524-6b0e8cf19ee1.pdf"
  code_link: 
  technique: Algorithm-level
  target: Efficient Training
  domain: Vision Recognition

- title: "CLOSE: Curriculum Learning On the Sharing Extent Towards Better One-shot NAS"
  authors: Zixuan Zhou*, Xuefei Ning*+, Yi Cai, Jiashu Han, Yiping Deng, Yuhan Dong, Huazhong Yang, Yu Wang+
  conference: ECCV 2022
  paper_link: "https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2F4a169027-0719-45e8-bafd-78535c8abcf6.pdf"
  code_link: 
  technique: Model-level (Structure Optimization)
  target: Efficient Optimization Process
  domain: Vision Recognition

- title: "TA-GATES: An Encoding Scheme for Neural Network Architectures"
  authors: Xuefei Ning*+, Zixuan Zhou*, Junbo Zhao, Tianchen Zhao, Yiping Deng, Changcheng Tang, Shuang Liang, Huazhong Yang, Yu Wang+
  conference: NeurIPS 2022 (Spotlight)
  paper_link: "https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2Fafc3d0ea-87b7-4277-9c37-54e294512390.pdf"
  code_link: 
  technique: Model-level (Structure Optimization)
  target: Efficient Optimization Process
  domain: Vision Recognition

- title: "Hardware Design and Software Practices for Efficient Neural Network Inference"
  authors: Yu Wang, Xuefei Ning, Shulin Zeng, Yi Cai, Kaiyuan Guo, Hanbo Sun, Changcheng Tang, Tianyi Lu, Shuang Liang, Tianchen Zhao
  conference: "Low-Power CV 2022"
  paper_link: "https://www.taylorfrancis.com/chapters/edit/10.1201/9781003162810-4/hardware-design-software-practices-efficient-neural-network-inference-yu-wang-xuefei-ning-shulin-zeng-yi-cai-kaiyuan-guo-hanbo-sun-changcheng-tang-tianyi-lu-shuang-liang-tianchen-zhao"
  technique: Model-level (Structure Optimization), Model-level (Quantization), System-level
  target: "Efficient Inference"
  domain: "Vision Recognition"

- title: "Machine learning for electronic design automation: A survey"
  authors: Guyue Huang, Jingbo Hu, Yifan He, Jialong Liu, Mingyuan Ma, Zhaoyang Shen, Juejian Wu, Yuanfan Xu, Hengrui Zhang, Kai Zhong, Xuefei Ning, Yuzhe Ma, Haoyu Yang, Bei Yu, Huazhong Yang, Yu Wang
  conference: TODAES 2021
  paper_link: "https://arxiv.org/pdf/2102.03357"
  code_link: "https://github.com/thu-nics/awesome_ai4eda"
  domain: "Other"

- title: "Evaluating Efficient Performance Estimators of Neural Architectures"
  authors: Xuefei Ning+, Changcheng Tang, Wenshuo Li, Zixuan Zhou, Shuang Liang, Huazhong Yang+, Yu Wang+
  conference: NeurIPS 2021
  paper_link: "https://proceedings.neurips.cc/paper/2021/file/65d90fc6d307590b14e9e1800d4e8eab-Paper.pdf"
  code_link: "https://github.com/walkerning/aw_nas/tree/master/examples/research/surgery"
  technique: Model-level (Structure Optimization)
  target: Efficient Optimization Process
  domain: Vision Recognition
  special_label: Evaluation

- title: "Black Box Search Space Profiling for Accelerator-Aware Neural Architecture Search"
  authors: Shulin Zeng, Hanbo Sun, Yu Xing, Xuefei Ning, Yi Shan, Xiaoming Chen, Yu Wang, Huazhong Yang
  conference: ASP-DAC 2020
  paper_link: "https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2Fbfc28f78-9c63-46b2-82d5-5fad5d540087.pdf"
  code_link: "https://github.com/walkerning/aw_nas/tree/master/examples/research/bbssp"
  technique: Model-level (Structure Optimization)
  target: Efficient Optimization Process, Efficient Inference
  domain: Vision Recognition

- title: "A Generic Graph-based Neural Architecture Encoding Scheme for Predictor-based NAS"
  authors: Xuefei Ning, Yin Zheng, Tianchen Zhao, Yu Wang, Huazhong Yang
  conference: ECCV 2020
  paper_link: "https://arxiv.org/pdf/2004.01899"
  code_link: "https://github.com/walkerning/aw_nas/tree/master/examples/research/gates"
  technique: Model-level (Structure Optimization)
  target: Efficient Optimization Process
  domain: Vision Recognition

- title: "DSA: More Efficient Budgeted Pruning via Differentiable Sparsity Allocation"
  authors: Xuefei Ning*, Tianchen Zhao*, Wenshuo Li, Peng Lei, Yu Wang, Huazhong Yang
  conference: ECCV 2020 (Spotlight)
  paper_link: "https://arxiv.org/pdf/2004.02164"
  code_link: 
  technique: Model-level (Structure Optimization)
  target: Efficient Inference
  domain: Vision Recognition

- title: "aw_nas: A Modularized and Extensible NAS framework"
  authors: "Xuefei Ning, Changcheng Tang, Wenshuo Li, Songyi Yang, Tianchen Zhao, Niansong Zhang, Tianyi Lu, Shuang Liang, Huazhong Yang, Yu Wang"
  conference: ArXiv 2020
  paper_link: "https://arxiv.org/abs/2012.10388"
  code_link: "https://github.com/walkerning/aw_nas/"
  technique: Model-level (Structure Optimization)
  target: Efficient Inference, Efficient Optimization Process
  domain: Vision Recognition, Language
