- time: 2024/06/24
  text: 'Our paper <a href="https://arxiv.org/abs/2406.14629" target="_blank" style="color: #0c53a5;">Can LLMs Learn by Teaching? A Preliminary Study</a> is public on arXiv. We make the first attempt in adapting the "learning by teach" strategy in education into LLMs and see if the contemporary LLMs are ready to learn by teach to improve their reasoning outcome or ability. The results are mixed but indeed show some promise. We also provide a roadmap for future research. Welcome to check the <a href="https://github.com/imagination-research/lbt" target="_blank" style="color: #0c53a5;">code</a>'

- time: 2024/06/24
  text: 'Our paper Mixture-of-Sparse-Attention (MoA) is avaliable on <a href="https://arxiv.org/abs/2406.12345" target="_blank" style="color: #0c53a5;">arXiv</a>. MoA compresses attention in LLMs, so that they can compute short attention, but remember long context. It achieves 5.5-6.7x faster throughput than dense FlashAttention2, improving retrieval accuracy by 1.5-7.1x compared to uniform sparse attention. Welcome to check the <a href="https://github.com/thu-nics/MoA" target="_blank" style="color: #0c53a5;">code</a>.'

- time: 2024/06/14
  text: 'Our paper <a href="https://arxiv.org/abs/2406.08552" target="_blank" style="color: #0c53a5;">DiTFastAttn: Attention Compression for Diffusion Transformer Models</a> is public on arXiv. We design three training-free techniques to compress the attention operation, which is the efficiency bottleneck when generating large-resolution images. For example, applying DiTFastAttn to PixArt-Sigma-XL can reduce the FLOPs and latency of attention computation by 88% and 37% when generating 2Kx2K images. Welcome to check the <a href="https://github.com/thu-nics/DiTFastAttn" target="_blank" style="color: #0c53a5;">code</a>.'

- time: 2024/06/02
  text: 'Our two papers are public on arXiv. In the first paper, we propose a mixed-precision quantization framework (MixDQ) that successfully tackles the challenging few-step text-to-image diffusion model quantization. With negligible visual quality degradation and content change, MixDQ could achieve W4A8, equivalent to 3.4x memory compression and 1.5x latency speedup. Check our <a href="https://arxiv.org/abs/2405.17873" target="_blank" style="color: #0c53a5;">paper</a> and <a href="https://github.com/A-suozhang/MixDQ" target="_blank" style="color: #0c53a5;">code(coming soon)</a> and <a href="https://a-suozhang.xyz/mixdq.github.io/" target="_blank" style="color: #0c53a5;">project page</a>. The second paper introduces ViDiT-Q, a quantization method specialized for the transformer-based video & image diffusion models. For popular large-scale models (e.g., open-sora, Latte, Pixart) for the video and image generation task, ViDiT-Q could achieve W8A8 quantization without metric degradation, and W4A8 without notable visual quality degradation. Check our <a href="https://arxiv.org/abs/2406.02540" target="_blank" style="color: #0c53a5;">paper</a> and <a href="https://github.com/A-suozhang/ViDiT-Q" target="_blank" style="color: #0c53a5;">code(coming soon)</a> and <a href="https://a-suozhang.xyz/viditq.github.io/" target="_blank" style="color: #0c53a5;">project page</a>.'

- time: 2024/04/23
  text: 'Our survey on efficient LLM inference is public on <a href="https://arxiv.org/pdf/2404.14294" target="_blank" style="color: #0c53a5;">arXiv</a>. Any discussions and suggestions are welcome!'

- time: 2024/04/05
  text: 'Our paper on more efficient “training” of consistency models is public on arXiv. This work proposes a method, Linear Combination of Saved Checkpoints (LCSC). LCSC uses gradient-free search-based checkpoint combination to obtain the final weights, achieving significant training speedups (23x on CIFAR-10 and 15x on ImageNet-64) compared to full gradient-based training. LCSC can be used to enhance pre-trained models with a small cost. Check our <a href="https://arxiv.org/abs/2404.02241" target="_blank" style="color: #0c53a5;">paper</a> and <a href="https://github.com/imagination-research/LCSC" target="_blank" style="color: #0c53a5;">code</a>.'

- time: 2024/05/02
  text: '1 paper, <a href="https://arxiv.org/abs/2402.18158" target="_blank" style="color: #0c53a5;">QLLM-Eval</a>, is accepted by ICML''24. This work evaluates 11 LLM families, different tasks (including emergent abilities, dialogue, long-context tasks, and so on), and different tensor types (Weight, Weight-Activation, Key-Value Cache). We provide quantative suggestions and qualitative insights on the quantization. Practitioners could benefit from this work with a full scope of quantization suggestions.'

- time: 2024/02/27
  text: '1 paper, <a href="https://arxiv.org/abs/2403.16379" target="_blank" style="color: #0c53a5;">FlashEval</a>, is accepted by CVPR''24. This work is on selecting a compact data subset to evaluate text-to-image Diffusion models.'

- time: 2024/02/09
  text: 'Our paper on long-context benchmark, LV-Eval, is public on <a href="https://arxiv.org/abs/2402.05136" target="_blank" style="color: #0c53a5;">arXiv</a>. Check the <a href="https://github.com/infinigence/LVEval" target="_blank" style="color: #0c53a5;">code</a>.'

- time: 2024/01/17
  text: '2 papers are acccepted by ICLR''24. One is <a href="https://arxiv.org/abs/2307.15337" target="_blank" style="color: #0c53a5;">Skeleton-of-Thoughts</a>, which accelerates LLM generation by letting the LLM itself to plan and generate segments in parallel, achieving ~2x speed-ups; Another is <a href="https://arxiv.org/abs/2312.07243" target="_blank" style="color: #0c53a5;">USF</a>, which summarizes the sampling strategies for diffusion and search for the best sampling strategy.'
