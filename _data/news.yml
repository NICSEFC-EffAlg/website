- time: 2024/06/02
  text: 'Our two papers are public on arXiv. In the first paper, we propose a mixed-precision quantization framework (MixDQ) that successfully tackles the challenging few-step text-to-image diffusion model quantization. With negligible visual quality degradation and content change, MixDQ could achieve W4A8, equivalent to 3.4x memory compression and 1.5x latency speedup. Check our <a href="https://arxiv.org/abs/2405.17873" target="_blank" style="color: #0c53a5;">paper</a> and <a href="https://github.com/A-suozhang/MixDQ" target="_blank" style="color: #0c53a5;">code(coming soon)</a> and <a href="https://a-suozhang.xyz/mixdq.github.io/" target="_blank" style="color: #0c53a5;">project page</a>. The second paper introduces ViDiT-Q, a quantization method specialized for the transformer-based video & image diffusion models. For popular large-scale models (e.g., open-sora, Latte, Pixart) for the video and image generation task, ViDiT-Q could achieve W8A8 quantization without metric degradation, and W4A8 without notable visual quality degradation. Check our <a href="https://arxiv.org/abs/2406.02540" target="_blank" style="color: #0c53a5;">paper</a> and <a href="https://github.com/A-suozhang/ViDiT-Q" target="_blank" style="color: #0c53a5;">code(coming soon)</a> and <a href="https://a-suozhang.xyz/viditq.github.io/" target="_blank" style="color: #0c53a5;">project page</a>.'

- time: 2024/04/23
  text: 'Our survey on efficient LLM inference is public on <a href="https://arxiv.org/pdf/2404.14294" target="_blank" style="color: #0c53a5;">arXiv</a>. Any discussions and suggestions are welcome!'

- time: 2024/04/05
  text: 'Our paper on more efficient “training” of consistency models is public on arXiv. This work proposes a method, Linear Combination of Saved Checkpoints (LCSC). LCSC uses gradient-free search-based checkpoint combination to obtain the final weights, achieving significant training speedups (23x on CIFAR-10 and 15x on ImageNet-64) compared to full gradient-based training. LCSC can be used to enhance pre-trained models with a small cost. Check our <a href="https://arxiv.org/abs/2404.02241" target="_blank" style="color: #0c53a5;">paper</a> and <a href="https://github.com/imagination-research/LCSC" target="_blank" style="color: #0c53a5;">code</a>.'

- time: 2024/05/02
  text: '1 paper, <a href="https://arxiv.org/abs/2402.18158" target="_blank" style="color: #0c53a5;">QLLM-Eval</a>, is accepted by ICML''24. This work evaluates 11 LLM families, different tasks (including emergent abilities, dialogue, long-context tasks, and so on), and different tensor types (Weight, Weight-Activation, Key-Value Cache). We provide quantative suggestions and qualitative insights on the quantization. Practitioners could benefit from this work with a full scope of quantization suggestions.'

- time: 2024/02/27
  text: '1 paper, <a href="https://arxiv.org/abs/2403.16379" target="_blank" style="color: #0c53a5;">FlashEval</a>, is accepted by CVPR''24. This work is on selecting a compact data subset to evaluate text-to-image Diffusion models.'

- time: 2024/02/09
  text: 'Our paper on long-context benchmark, LV-Eval, is public on <a href="https://arxiv.org/abs/2402.05136" target="_blank" style="color: #0c53a5;">arXiv</a>. Check the <a href="https://github.com/infinigence/LVEval" target="_blank" style="color: #0c53a5;">code</a>.'

- time: 2024/01/17
  text: '2 papers are acccepted by ICLR''24. One is <a href="https://arxiv.org/abs/2307.15337" target="_blank" style="color: #0c53a5;">Skeleton-of-Thoughts</a>, which accelerates LLM generation by letting the LLM itself to plan and generate segments in parallel, achieving ~2x speed-ups; Another is <a href="https://arxiv.org/abs/2312.07243" target="_blank" style="color: #0c53a5;">USF</a>, which summarizes the sampling strategies for diffusion and search for the best sampling strategy.'
