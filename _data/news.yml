- time: 2024/11/23
  text: Give an invited talk about efficient AIGC research at UESTC.
  
- time: 2024/11/20
  text: 'Give an invited <a href="https://www.bilibili.com/video/BV1XUURYzEgR" target="_blank" style="color: #0c53a5;">talk</a> about our EffAlg team and recent researches at AI Time.'

- time: 2024/11/06
  text: 'Will serve as a TPC member for DAC 2025''s AI Track.'

- time: 2024/11/06
  text: Give an invited talk about efficient AIGC research at HUAWEI 2012 Labs Global Software Technology Summit.

- time: 2024/10/20
  text: Get the CCF-Baidu Open Faculty Research Fund.

- time: 2024/10/17
  text: Will serve as an Area Chair for CVPR 2025.

- time: 2024/09/26
  # text: 'Three of our papers are accepted by NeurIPS 2024! (1) "<a href="https://arxiv.org/abs/2406.14629" target="_blank" style="color: #0c53a5;">Can LLMs Learn by Teaching? A Preliminary Study</a>" explores a novel pathway (learning by teach) towards better and continual evolving reasoning ability of LLMs. (2) "<a href="https://arxiv.org/abs/2406.08552" target="_blank" style="color: #0c53a5;">DiTFastAttn: Attention Compression for Diffusion Transformer Models</a>" proposes post-training attention sparsification and sharing techniques to accelerate Diffusion Transformers. (3) "Rad-NeRF: Ray-decoupled Training of Neural Radiance Field" proposes a ray-decopuled soft ensemble of NeRF MLPs to better model complex scenes, and offers a new and parameter-efficient scaling dimension.'
  text: '3 papers, <a href="https://arxiv.org/abs/2406.14629" target="_blank" style="color: #0c53a5;">LbT</a>, <a href="https://arxiv.org/abs/2406.08552" target="_blank" style="color: #0c53a5;">DiTFastAttn</a>, and <a href="https://openreview.net/pdf?id=nBrnfYeKf9" target="_blank" style="color: #0c53a5;">Rad-Nerf</a>, are accepted by NeurIPS 2024.'

- time: 2024/07/11
  text: 'Give 2 invited <a href="https://nics-effalg.com/assets/ppt/2024-10-18-GMCA.pdf" target="_blank" style="color: #0c53a5;">talks</a> about efficient AIGC research at Apple China and CAS.'
  
- time: 2024/07/02
  text: '1 paper, <a href="https://arxiv.org/abs/2405.17873" target="_blank" style="color: #0c53a5;">MixDQ</a>, is accepted by ECCV'24.'

- time: 2024/07/01
  text: Will serve as a Senior Area Chair for ACL 2025.

- time: 2024/07/01
  # text: 'Our paper <a href="https://arxiv.org/abs/2407.00945" target="_blank" style="color: #0c53a5;">Efficient Expert Pruning for Sparse Mixture-of-Experts Language Models: Enhancing Performance and Reducing Inference Costs</a> is public on arXiv. We conduct expert pruning of MoE models and use weight merging to inherit the knowledge from the retrained and discarded experts. This is a novel and gradient-free way to inherit the knowledge during model compression. The code will soon be available at <a href="https://github.com/imagination-research/EEP" target="_blank" style="color: #0c53a5;">code</a>.'
  text: '1 paper, <a href="https://arxiv.org/abs/2407.00945" target="_blank" style="color: #0c53a5;">EEP</a>, is public on arXiv.'

# - time: 2024/06/24
#   text: 'Our paper, <a href="https://arxiv.org/abs/2406.14629" target="_blank" style="color: #0c53a5;">Can LLMs Learn by Teaching? A Preliminary Study</a> is public on arXiv. We make the first attempt in adapting the "learning by teach" strategy in education into LLMs and see if the contemporary LLMs are ready to learn by teach to improve the outcome or ability for mathematical reasoning and code synthesis. The results show some promise. We also provide a roadmap for future research. Welcome to check the <a href="https://github.com/imagination-research/lbt" target="_blank" style="color: #0c53a5;">code</a>.'

- time: 2024/06/24
  # text: 'Our paper <a href="https://arxiv.org/abs/2406.14909" target="_blank" style="color: #0c53a5;">MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression</a> is public on arXiv. MoA compresses attention in LLMs, so that they can compute short attention, but remember long context. It achieves 5.5-6.7x faster throughput than dense FlashAttention2, improving retrieval accuracy by 1.5-7.1x compared to uniform sparse attention. Welcome to check the <a href="https://github.com/thu-nics/MoA" target="_blank" style="color: #0c53a5;">code</a>.'
  text: '1 paper, <a href="https://arxiv.org/abs/2406.14909" target="_blank" style="color: #0c53a5;">MoA</a>, is public on arXiv.'

# - time: 2024/06/14
#   text: 'Our paper <a href="https://arxiv.org/abs/2406.08552" target="_blank" style="color: #0c53a5;">DiTFastAttn: Attention Compression for Diffusion Transformer Models</a> is public on arXiv. We design three training-free techniques to compress the attention operation, which is the efficiency bottleneck when generating large-resolution images. For example, applying DiTFastAttn to PixArt-Sigma-XL can reduce the FLOPs and latency of attention computation by 88% and 37% when generating 2Kx2K images. Welcome to check the <a href="https://github.com/thu-nics/DiTFastAttn" target="_blank" style="color: #0c53a5;">code</a>.'

- time: 2024/06/02
  # text: 'Our 2 papers are public on arXiv. In the first paper, we propose <a href="https://arxiv.org/abs/2405.17873" target="_blank" style="color: #0c53a5;">MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization</a> that successfully tackles the challenging few-step text-to-image diffusion model quantization. With negligible visual quality degradation and content change, MixDQ could achieve W4A8, equivalent to 3.4x memory compression and 1.5x latency speedup. Check our <a href="https://github.com/A-suozhang/MixDQ" target="_blank" style="color: #0c53a5;">code</a> and <a href="https://a-suozhang.xyz/mixdq.github.io/" target="_blank" style="color: #0c53a5;">project page</a>. The second paper is <a href="https://arxiv.org/abs/2406.02540v1" target="_blank" style="color: #0c53a5;">ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation</a>, a quantization method specialized for the transformer-based video & image diffusion models. For popular large-scale models (e.g., open-sora, Latte, Pixart) for the video and image generation task, ViDiT-Q could achieve W8A8 quantization without metric degradation, and W4A8 without notable visual quality degradation. Check our <a href="https://github.com/A-suozhang/ViDiT-Q" target="_blank" style="color: #0c53a5;">code</a> and <a href="https://a-suozhang.xyz/viditq.github.io/" target="_blank" style="color: #0c53a5;">project page</a>. <strong>Update 07/01:</strong> MixDQ is accepted by ECCV''24!'
  text: '1 paper, <a href="https://arxiv.org/abs/2406.02540v1" target="_blank" style="color: #0c53a5;">ViDiT-Q</a>, are public on arXiv.'

- time: 2024/04/23
  # text: 'Our survey <a href="https://arxiv.org/pdf/2404.14294" target="_blank" style="color: #0c53a5;">A Survey on Efficient Inference for Large Language Models</a> is public on arXiv. Any discussions and suggestions are welcome!'
  text: '1 survey, <a href="https://arxiv.org/pdf/2404.14294" target="_blank" style="color: #0c53a5;">Efficient Inference for Large Language Models</a>, is public on arXiv. Any suggestions are welcome!'

- time: 2024/04/05
  # text: 'Our paper <a href="https://arxiv.org/abs/2404.02241" target="_blank" style="color: #0c53a5;">Linear Combination of Saved Checkpoints Makes Consistency and Diffusion Models Better</a> is public on arXiv. This work proposes a method, Linear Combination of Saved Checkpoints (LCSC). LCSC uses gradient-free search-based checkpoint combination to obtain the final weights, achieving significant training speedups (23x on CIFAR-10 and 15x on ImageNet-64) compared to full gradient-based training. LCSC can be used to enhance pre-trained models with a small cost. Check our <a href="https://github.com/imagination-research/LCSC" target="_blank" style="color: #0c53a5;">code</a>.'
  text: '1 paper, <a href="https://arxiv.org/abs/2404.02241" target="_blank" style="color: #0c53a5;">LCSC</a>, is public on arXiv.'

- time: 2024/05/02
  # text: '1 paper, <a href="https://arxiv.org/abs/2402.18158" target="_blank" style="color: #0c53a5;">Evaluating Quantized Large Language Models</a>, is accepted by ICML''24. This work evaluates 11 LLM families, different tasks (including emergent abilities, dialogue, long-context tasks, and so on), and different tensor types (Weight, Weight-Activation, Key-Value Cache). We provide quantative suggestions and qualitative insights on the quantization. Practitioners could benefit from this work with a full scope of quantization suggestions.'
  text: '1 paper, <a href="https://arxiv.org/abs/2402.18158" target="_blank" style="color: #0c53a5;">QLLM-Eval</a>, is accepted by ICML''24.'

- time: 2024/02/27
  # text: '1 paper, <a href="https://arxiv.org/abs/2403.16379" target="_blank" style="color: #0c53a5;">FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models</a>, is accepted by CVPR''24. This work is on selecting a compact data subset to evaluate text-to-image Diffusion models.'
  text: '1 paper, <a href="https://arxiv.org/abs/2403.16379" target="_blank" style="color: #0c53a5;">FlashEval</a>, is accepted by CVPR''24.'

- time: 2024/02/09
  # text: 'Our paper on long-context benchmark, <a href="https://arxiv.org/abs/2402.05136" target="_blank" style="color: #0c53a5;">LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K</a>, is public on arXiv. Check the <a href="https://github.com/infinigence/LVEval" target="_blank" style="color: #0c53a5;">code</a>.'
  text: '1 benchmark, <a href="https://arxiv.org/abs/2402.05136" target="_blank" style="color: #0c53a5;">LV-Eval</a>, is public on arXiv. Check the <a href="https://github.com/infinigence/LVEval" target="_blank" style="color: #0c53a5;">code</a>.'

- time: 2024/01/17
  # text: '2 papers are acccepted by ICLR''24. One is <a href="https://arxiv.org/abs/2307.15337" target="_blank" style="color: #0c53a5;">Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation</a>, which accelerates LLM generation by letting the LLM itself to plan and generate segments in parallel, achieving ~2x speed-ups; Another is <a href="https://arxiv.org/abs/2312.07243" target="_blank" style="color: #0c53a5;">A Unified Sampling Framework for Solver Searching of Diffusion Probabilistic Models</a>, which summarizes the sampling strategies for diffusion and search for the best sampling strategy.'
  text: '2 papers, <a href="https://arxiv.org/abs/2307.15337" target="_blank" style="color: #0c53a5;">Skeleton-of-Thought</a> and <a href="https://arxiv.org/abs/2312.07243" target="_blank" style="color: #0c53a5;">USF</a>, are acccepted by ICLR''24.'
